{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T04:13:57.909677Z",
     "start_time": "2025-10-26T04:13:12.731225Z"
    }
   },
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "点云 + 深度学习 论文自动检索（终极稳定版）\n",
    "支持：\n",
    "  • Google Scholar  → SerpApi（防封、稳定）\n",
    "  • arXiv          → 官方 API\n",
    "输出：CSV + BibTeX（高被引优先）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- API ----------\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "# ---------- 解析 ----------\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "\n",
    "# ==================== 配置区 ====================\n",
    "\n",
    "# SerpApi Key\n",
    "SERPAPI_KEY = \"63eff930ec8b25bae49e14f4d4fd88bd19ab60d118cbf644e63082814710aede\"\n",
    "\n",
    "QUERIES = {\n",
    "    \"scholar\": [\n",
    "        '\"point cloud\" \"deep learning\" (classification OR segmentation OR registration)',\n",
    "        '\"point cloud\" \"PointNet\" OR \"PointNet++\" OR \"RandLA-Net\" OR \"Point Transformer\"'\n",
    "    ],\n",
    "    \"arxiv\": [\n",
    "        \"point cloud deep learning\",\n",
    "        \"PointNet OR PointNet++ OR RandLA-Net OR Point Transformer\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "MIN_YEAR = 2016\n",
    "MIN_CITATIONS_SCHOLAR = 50\n",
    "MAX_PAGES = {\"scholar\": 3, \"arxiv\": 3}\n",
    "OUTPUT_DIR = \"retrieved_papers\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===============================================\n",
    "\n",
    "def random_sleep(min_sec=2, max_sec=5):\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "\n",
    "# ------------------- Google Scholar (SerpApi) -------------------\n",
    "def crawl_google_scholar_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key or not api_key.strip():\n",
    "        print(\"SerpApi Key 无效或未提供，跳过 Google Scholar\")\n",
    "        return []\n",
    "\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"google_scholar\",\n",
    "            \"q\": query,\n",
    "            \"hl\": \"en\",\n",
    "            \"start\": page * 10,\n",
    "            \"num\": 10,\n",
    "            \"as_ylo\": MIN_YEAR,\n",
    "            \"api_key\": api_key\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(base_url, params=params, timeout=30)\n",
    "            data = resp.json()\n",
    "\n",
    "            if \"organic_results\" not in data:\n",
    "                print(f\"  第 {page+1} 页无结果或 API 限流\")\n",
    "                break\n",
    "\n",
    "            page_count = 0\n",
    "            for item in data[\"organic_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown Title\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"snippet\", \"\")\n",
    "\n",
    "                    # ---------- 引用数 ----------\n",
    "                    cited = 0\n",
    "                    inline = item.get(\"inline_links\", {})\n",
    "                    if inline.get(\"cited_by\"):\n",
    "                        txt = inline[\"cited_by\"].get(\"total\", \"0\")\n",
    "                        if txt:\n",
    "                            cited = int(\"\".join(filter(str.isdigit, str(txt))))\n",
    "\n",
    "                    # ---------- 年份（彻底防御性） ----------\n",
    "                    year = None\n",
    "                    pub_info = item.get(\"publication_info\", {}).get(\"summary\", \"\")\n",
    "\n",
    "                    # 防御：pub_info 可能是 int、None、dict 等\n",
    "                    if pub_info is None:\n",
    "                        pub_info = \"\"\n",
    "                    elif isinstance(pub_info, (int, float)):\n",
    "                        pub_info = str(int(pub_info))\n",
    "                    elif isinstance(pub_info, dict):\n",
    "                        pub_info = pub_info.get(\"summary\", \"\") or \"\"\n",
    "                    elif not isinstance(pub_info, str):\n",
    "                        pub_info = str(pub_info)\n",
    "\n",
    "                    # 提取年份\n",
    "                    for token in pub_info.replace(\",\", \" \").split():\n",
    "                        token = token.strip()\n",
    "                        if token.isdigit() and 1900 < int(token) < 2100:\n",
    "                            year = int(token)\n",
    "                            break\n",
    "\n",
    "                    # ---------- 过滤条件 ----------\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        # 作者与期刊分离\n",
    "                        parts = str(pub_info).split(\" - \", 1)\n",
    "                        authors = parts[0].strip() if len(parts) > 0 else \"Unknown\"\n",
    "                        venue = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "                        papers.append({\n",
    "                            \"title\": title,\n",
    "                            \"authors\": authors,\n",
    "                            \"year\": year,\n",
    "                            \"venue\": venue,\n",
    "                            \"citations\": cited,\n",
    "                            \"abstract\": snippet[:500],\n",
    "                            \"pdf_link\": link,\n",
    "                            \"source\": \"Google Scholar\"\n",
    "                        })\n",
    "                        page_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  解析单篇失败: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"  第 {page+1} 页抓取 {len(data['organic_results'])} 篇，成功添加 {page_count} 篇，累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  SerpApi 请求异常: {e}\")\n",
    "            break\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "# ------------------- arXiv API -------------------\n",
    "def crawl_arxiv_api(query, max_pages=3, max_results_per_page=50):\n",
    "    papers = []\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    start = 0\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"search_query\": f\"all:{query}\",\n",
    "            \"start\": start,\n",
    "            \"max_results\": max_results_per_page,\n",
    "            \"sortBy\": \"submittedDate\",\n",
    "            \"sortOrder\": \"descending\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(base_url, params=params, timeout=15)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"  arXiv API 错误: {resp.status_code}\")\n",
    "                break\n",
    "\n",
    "            feed = feedparser.parse(resp.content)\n",
    "            if not feed.entries:\n",
    "                print(f\"  第 {page+1} 页无结果\")\n",
    "                break\n",
    "\n",
    "            page_count = 0\n",
    "            for entry in feed.entries:\n",
    "                try:\n",
    "                    title = entry.get(\"title\", \"Unknown\").strip()\n",
    "                    authors = \", \".join([a.get(\"name\", \"\") for a in entry.get(\"authors\", []) if a.get(\"name\")]) or \"Unknown\"\n",
    "                    abstract = (entry.get(\"summary\", \"\") or \"\").replace(\"\\n\", \" \")[:500]\n",
    "                    abs_link = entry.get(\"link\", \"\")\n",
    "                    pdf_link = abs_link.replace(\"/abs/\", \"/pdf/\") + \".pdf\" if \"/abs/\" in abs_link else \"\"\n",
    "\n",
    "                    # 年份\n",
    "                    year = None\n",
    "                    for field in (\"published\", \"updated\"):\n",
    "                        d = entry.get(field, \"\")\n",
    "                        if d and len(d) >= 4:\n",
    "                            try:\n",
    "                                year = int(d[:4])\n",
    "                                break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                    if not year or year < 1900:\n",
    "                        year = 2025\n",
    "\n",
    "                    if year >= MIN_YEAR:\n",
    "                        papers.append({\n",
    "                            \"title\": title,\n",
    "                            \"authors\": authors,\n",
    "                            \"year\": year,\n",
    "                            \"venue\": \"arXiv\",\n",
    "                            \"citations\": 0,\n",
    "                            \"abstract\": abstract,\n",
    "                            \"pdf_link\": pdf_link,\n",
    "                            \"source\": \"arXiv\"\n",
    "                        })\n",
    "                        page_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  解析 arXiv 单篇失败: {e}\")\n",
    "                    continue\n",
    "\n",
    "            print(f\"  第 {page+1} 页抓取 {len(feed.entries)} 条，成功添加 {page_count} 篇，累计 {len(papers)} 篇\")\n",
    "            start += max_results_per_page\n",
    "            time.sleep(3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  arXiv 请求异常: {e}\")\n",
    "            break\n",
    "\n",
    "    return papers\n",
    "\n",
    "\n",
    "# ------------------- 主函数 -------------------\n",
    "def main():\n",
    "    all_papers = []\n",
    "\n",
    "    # ---------- Google Scholar ----------\n",
    "    print(\"开始检索 Google Scholar...\")\n",
    "    for q in QUERIES[\"scholar\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_google_scholar_api(q, MAX_PAGES[\"scholar\"], SERPAPI_KEY))\n",
    "\n",
    "    # ---------- arXiv ----------\n",
    "    print(\"\\n开始检索 arXiv...\")\n",
    "    for q in QUERIES[\"arxiv\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_arxiv_api(q, MAX_PAGES[\"arxiv\"]))\n",
    "\n",
    "    # ---------- 字段统一 ----------\n",
    "    clean = []\n",
    "    for p in all_papers:\n",
    "        try:\n",
    "            clean.append({\n",
    "                \"title\": str(p.get(\"title\", \"Unknown Title\")),\n",
    "                \"authors\": str(p.get(\"authors\", \"Unknown\")),\n",
    "                \"year\": int(p.get(\"year\") or 0),\n",
    "                \"venue\": str(p.get(\"venue\", \"Unknown\")),\n",
    "                \"citations\": int(p.get(\"citations\") or 0),\n",
    "                \"abstract\": str(p.get(\"abstract\", \"\"))[:500],\n",
    "                \"pdf_link\": str(p.get(\"pdf_link\", \"\")),\n",
    "                \"source\": str(p.get(\"source\", \"Unknown\"))\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"  字段清洗失败: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not clean:\n",
    "        print(\"警告：未抓取到任何有效论文！\")\n",
    "        return\n",
    "\n",
    "    # ---------- 数据处理 ----------\n",
    "    df = pd.DataFrame(clean)\n",
    "    df.drop_duplicates(subset=[\"title\"], inplace=True)\n",
    "    df = df[df[\"year\"] >= MIN_YEAR]\n",
    "    df.sort_values(by=[\"citations\", \"year\"], ascending=[False, False], inplace=True)\n",
    "\n",
    "    # ---------- 保存 ----------\n",
    "    ts = datetime.now().strftime(\"%Y%m%d\")\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"pointcloud_dl_papers_{ts}.csv\")\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"\\nCSV 保存: {csv_path}，共 {len(df)} 篇\")\n",
    "\n",
    "    # ---------- BibTeX ----------\n",
    "    db = BibDatabase()\n",
    "    db.entries = []\n",
    "    for _, row in df.iterrows():\n",
    "        author_str = row[\"authors\"].replace(\", \", \" and \")\n",
    "        first = \"\".join(c for c in row[\"authors\"].split(\",\")[0].lower() if c.isalnum()) or \"unknown\"\n",
    "\n",
    "        entry = {\n",
    "            \"ENTRYTYPE\": \"misc\",\n",
    "            \"ID\": f\"{first}{row['year']}\",\n",
    "            \"title\": row[\"title\"],\n",
    "            \"author\": author_str,\n",
    "            \"year\": str(row[\"year\"]),\n",
    "            \"url\": row[\"pdf_link\"],\n",
    "            \"note\": f\"[{row['source']}] Citations: {row['citations']}\"\n",
    "        }\n",
    "        if \"arxiv\" in row[\"source\"].lower():\n",
    "            entry[\"howpublished\"] = f\"\\\\url{{{row['pdf_link']}}}\"\n",
    "        else:\n",
    "            entry[\"journal\"] = row[\"venue\"]\n",
    "        db.entries.append(entry)\n",
    "\n",
    "    bib_path = os.path.join(OUTPUT_DIR, f\"pointcloud_dl_papers_{ts}.bib\")\n",
    "    with open(bib_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        writer = BibTexWriter()\n",
    "        writer.indent = \"    \"\n",
    "        f.write(writer.write(db))\n",
    "    print(f\"BibTeX 保存: {bib_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始检索 Google Scholar...\n",
      "  查询: \"point cloud\" \"deep learning\" (classification OR segmentation OR registration)\n",
      "  第 1 页抓取 10 篇，成功添加 7 篇，累计 7 篇\n",
      "  第 2 页抓取 10 篇，成功添加 6 篇，累计 13 篇\n",
      "  第 3 页抓取 10 篇，成功添加 4 篇，累计 17 篇\n",
      "  查询: \"point cloud\" \"PointNet\" OR \"PointNet++\" OR \"RandLA-Net\" OR \"Point Transformer\"\n",
      "  第 1 页抓取 10 篇，成功添加 7 篇，累计 7 篇\n",
      "  第 2 页抓取 10 篇，成功添加 4 篇，累计 11 篇\n",
      "  第 3 页抓取 10 篇，成功添加 3 篇，累计 14 篇\n",
      "\n",
      "开始检索 arXiv...\n",
      "  查询: point cloud deep learning\n",
      "  第 1 页抓取 10 条，成功添加 10 篇，累计 10 篇\n",
      "  第 2 页无结果\n",
      "  查询: PointNet OR PointNet++ OR RandLA-Net OR Point Transformer\n",
      "  第 1 页抓取 50 条，成功添加 50 篇，累计 50 篇\n",
      "  第 2 页抓取 50 条，成功添加 50 篇，累计 100 篇\n",
      "  第 3 页抓取 50 条，成功添加 50 篇，累计 150 篇\n",
      "\n",
      "CSV 保存: retrieved_papers\\pointcloud_dl_papers_20251026.csv，共 189 篇\n",
      "BibTeX 保存: retrieved_papers\\pointcloud_dl_papers_20251026.bib\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T07:06:26.741236Z",
     "start_time": "2025-10-26T06:53:46.971522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import zipfile\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "# ==================== 配置区 ====================\n",
    "SERPAPI_KEY = \"63eff930ec8b25bae49e14f4d4fd88bd19ab60d118cbf644e63082814710aede\"\n",
    "\n",
    "# QUERIES = {\n",
    "#     \"scholar\": [\n",
    "#         '\"point cloud\" \"deep learning\" (classification OR segmentation OR registration)',\n",
    "#         '\"point cloud\" \"PointNet\" OR \"PointNet++\" OR \"RandLA-Net\" OR \"Point Transformer\"'\n",
    "#     ],\n",
    "#     \"arxiv\": [\n",
    "#         \"point cloud deep learning\",\n",
    "#         \"PointNet OR PointNet++ OR RandLA-Net OR Point Transformer\"\n",
    "#     ]\n",
    "# }\n",
    "# MIN_YEAR = 2016\n",
    "# MIN_CITATIONS_SCHOLAR = 50\n",
    "# MAX_PAGES = {\"scholar\": 3, \"arxiv\": 3}\n",
    "# TOP_N = 50\n",
    "QUERIES = {\n",
    "    # ------------------- Google Scholar -------------------\n",
    "    \"scholar\": [\n",
    "        # 1. 核心关键词组合（必须出现）\n",
    "        '\"ICESat-2\" (\"point cloud\" OR \"photon cloud\" OR lidar OR \"laser altimetry\")',\n",
    "        '\"ICESat-2\" \"deep learning\" (classification OR segmentation OR registration OR denoising OR bathymetry)',\n",
    "\n",
    "        # 2. 光子点云 + 深度学习\n",
    "        '\"photon point cloud\" \"deep learning\" (classification OR segmentation OR denoising OR bathymetry)',\n",
    "\n",
    "        # 3. 卫星激光点云处理\n",
    "        '\"satellite lidar\" OR \"spaceborne lidar\" (\"point cloud\" OR photon) \"deep learning\"',\n",
    "\n",
    "        # 4. 测深（bathymetry）相关\n",
    "        '\"ICESat-2\" (bathymetry OR \"water depth\" OR \"underwater topography\") \"deep learning\"',\n",
    "\n",
    "        # 5. 经典网络在 ICESat‑2 上的应用\n",
    "        '\"ICESat-2\" (\"PointNet\" OR \"PointNet++\" OR \"RandLA-Net\" OR \"Point Transformer\" OR DGCNN OR KPConv)',\n",
    "\n",
    "        # 6. 噪声去除 / 信号提取\n",
    "        '\"ICESat-2\" (denoising OR \"signal extraction\" OR \"photon classification\") \"deep learning\"'\n",
    "    ],\n",
    "\n",
    "    # ------------------- arXiv -------------------\n",
    "    \"arxiv\": [\n",
    "        # 1. 基础关键词\n",
    "        \"ICESat-2 point cloud OR photon cloud OR lidar\",\n",
    "        \"ICESat-2 deep learning classification OR segmentation OR bathymetry\",\n",
    "\n",
    "        # 2. 光子点云\n",
    "        \"photon point cloud deep learning\",\n",
    "\n",
    "        # 3. 卫星激光雷达\n",
    "        \"satellite lidar OR spaceborne lidar point cloud deep learning\",\n",
    "\n",
    "        # 4. 测深\n",
    "        \"ICESat-2 bathymetry OR water depth deep learning\",\n",
    "\n",
    "        # 5. 网络模型\n",
    "        \"ICESat-2 PointNet OR PointNet++ OR RandLA-Net OR Point Transformer\",\n",
    "\n",
    "        # 6. 去噪 / 分类\n",
    "        \"ICESat-2 denoising OR photon classification deep learning\"\n",
    "    ]\n",
    "}\n",
    "MIN_YEAR = 2018                  # ICESat-2 发射年份\n",
    "MIN_CITATIONS_SCHOLAR = 10       # 放宽引用要求（新领域论文少）\n",
    "MAX_PAGES = {\"scholar\": 5, \"arxiv\": 5}\n",
    "TOP_N = 100                      # 保留更多候选\n",
    "\n",
    "# 邮件配置（163 邮箱为例，推荐使用）\n",
    "EMAIL_SENDER = \"2117735297@qq.com\"          # 替换成你的新 QQ 邮箱\n",
    "EMAIL_PASSWORD = \"ebqipkzjqifbfcfi\"        # 替换成你的授权码\n",
    "EMAIL_RECEIVER = \"zhouyusen@nuaa.edu.cn\"\n",
    "# SMTP_SERVER = \"smtp.163.com\"\n",
    "# SMTP_PORT = 465\n",
    "\n",
    "OUTPUT_DIR = \"retrieved_papers\"\n",
    "PDF_DIR = os.path.join(OUTPUT_DIR, \"pdfs\")\n",
    "ZIP_PATH = os.path.join(OUTPUT_DIR, \"pdfs.zip\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "# ===============================================\n",
    "\n",
    "def random_sleep(min_sec=2, max_sec=5):\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "def safe_filename(name):\n",
    "    name = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', '_', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name[:100]\n",
    "\n",
    "def download_pdf(url, filepath, retries=3):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=30, stream=True)\n",
    "            if resp.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in resp.iter_content(1024*1024):\n",
    "                        f.write(chunk)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"  [重试 {attempt+1}] {e}\")\n",
    "            time.sleep(2)\n",
    "    return False\n",
    "\n",
    "# ------------------- Google Scholar -------------------\n",
    "def crawl_google_scholar_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"google_scholar\", \"q\": query, \"hl\": \"en\",\n",
    "            \"start\": page * 10, \"num\": 10, \"as_ylo\": MIN_YEAR, \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"organic_results\" not in data: break\n",
    "            for item in data[\"organic_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"snippet\", \"\")\n",
    "                    cited = 0\n",
    "                    if item.get(\"inline_links\", {}).get(\"cited_by\"):\n",
    "                        txt = item[\"inline_links\"][\"cited_by\"].get(\"total\", \"0\")\n",
    "                        cited = int(\"\".join(filter(str.isdigit, str(txt)))) if txt else 0\n",
    "                    year = None\n",
    "                    pub_info = item.get(\"publication_info\", {}).get(\"summary\", \"\")\n",
    "                    if isinstance(pub_info, (int, float)): pub_info = str(int(pub_info))\n",
    "                    elif not isinstance(pub_info, str): pub_info = str(pub_info)\n",
    "                    for token in pub_info.replace(\",\", \" \").split():\n",
    "                        if token.isdigit() and 1900 < int(token) < 2100:\n",
    "                            year = int(token); break\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        parts = pub_info.split(\" - \", 1)\n",
    "                        authors = parts[0].strip() if len(parts) > 0 else \"Unknown\"\n",
    "                        venue = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": venue,\n",
    "                            \"citations\": cited, \"abstract\": snippet[:500], \"pdf_link\": link,\n",
    "                            \"source\": \"Google Scholar\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  Scholar 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- arXiv -------------------\n",
    "def crawl_arxiv_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"google_scholar\", \"q\": f\"{query} source:arxiv\", \"hl\": \"en\",\n",
    "            \"start\": page * 10, \"num\": 10, \"as_ylo\": MIN_YEAR, \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"organic_results\" not in data: break\n",
    "            for item in data[\"organic_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"snippet\", \"\")\n",
    "                    cited = 0\n",
    "                    if item.get(\"inline_links\", {}).get(\"cited_by\"):\n",
    "                        txt = item[\"inline_links\"][\"cited_by\"].get(\"total\", \"0\")\n",
    "                        cited = int(\"\".join(filter(str.isdigit, str(txt)))) if txt else 0\n",
    "                    year = None\n",
    "                    pub_info = item.get(\"publication_info\", {}).get(\"summary\", \"\")\n",
    "                    if isinstance(pub_info, (int, float)): pub_info = str(int(pub_info))\n",
    "                    elif not isinstance(pub_info, str): pub_info = str(pub_info)\n",
    "                    for token in pub_info.replace(\",\", \" \").split():\n",
    "                        if token.isdigit() and 1900 < int(token) < 2100:\n",
    "                            year = int(token); break\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        authors = \"Unknown\"\n",
    "                        if pub_info:\n",
    "                            parts = pub_info.split(\" - \", 1)\n",
    "                            authors = parts[0].strip() if len(parts) > 0 else \"Unknown\"\n",
    "                        pdf_link = \"\"\n",
    "                        if \"arxiv.org/abs/\" in link:\n",
    "                            arxiv_id = link.split(\"/abs/\")[-1].split(\"?\")[0].split(\"#\")[0]\n",
    "                            pdf_link = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "                        elif \"arxiv.org/pdf/\" in link:\n",
    "                            pdf_link = link\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": \"arXiv\",\n",
    "                            \"citations\": cited, \"abstract\": snippet[:500], \"pdf_link\": pdf_link or link,\n",
    "                            \"source\": \"arXiv\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  arXiv 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- 邮件发送 -------------------\n",
    "def send_email_with_attachments(csv_path, bib_path, zip_path):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_SENDER\n",
    "    msg['To'] = EMAIL_RECEIVER\n",
    "    msg['Subject'] = f\"【点云论文】Top {TOP_N} 高被引论文- {datetime.now().strftime('%Y%m%d')}\"\n",
    "\n",
    "    body = f\"附件为 Top {TOP_N} 篇高被引论文。\"\n",
    "    msg.attach(MIMEText(body, 'plain', 'utf-8'))\n",
    "\n",
    "    for file_path in [csv_path, bib_path, zip_path]:\n",
    "        if not os.path.exists(file_path): continue\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            part = MIMEBase('application', 'octet-stream')\n",
    "            part.set_payload(f.read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header('Content-Disposition', f'attachment; filename= {os.path.basename(file_path)}')\n",
    "            msg.attach(part)\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            print(f\"正在使用 QQ 邮箱发送（第 {attempt+1} 次）...\")\n",
    "            server = smtplib.SMTP(\"smtp.qq.com\", 587, timeout=20)\n",
    "            server.ehlo()\n",
    "            server.starttls()\n",
    "            server.ehlo()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.sendmail(EMAIL_SENDER, EMAIL_RECEIVER, msg.as_string())\n",
    "            server.quit()\n",
    "            print(f\"邮件发送成功！\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"失败: {e}\")\n",
    "            time.sleep(3)\n",
    "    print(\"发送失败！请检查 QQ 邮箱授权码\")\n",
    "\n",
    "# ------------------- 主函数 -------------------\n",
    "def main():\n",
    "    all_papers = []\n",
    "    print(\"开始检索 Google Scholar...\")\n",
    "    for q in QUERIES[\"scholar\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_google_scholar_api(q, MAX_PAGES[\"scholar\"], SERPAPI_KEY))\n",
    "\n",
    "    print(\"\\n开始检索 arXiv...\")\n",
    "    for q in QUERIES[\"arxiv\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_arxiv_api(q, MAX_PAGES[\"arxiv\"], SERPAPI_KEY))\n",
    "\n",
    "    clean = []\n",
    "    for p in all_papers:\n",
    "        try:\n",
    "            clean.append({\n",
    "                \"title\": str(p.get(\"title\", \"Unknown\")),\n",
    "                \"authors\": str(p.get(\"authors\", \"Unknown\")),\n",
    "                \"year\": int(p.get(\"year\") or 0),\n",
    "                \"venue\": str(p.get(\"venue\", \"Unknown\")),\n",
    "                \"citations\": int(p.get(\"citations\") or 0),\n",
    "                \"abstract\": str(p.get(\"abstract\", \"\"))[:500],\n",
    "                \"pdf_link\": str(p.get(\"pdf_link\", \"\")),\n",
    "                \"source\": str(p.get(\"source\", \"Unknown\"))\n",
    "            })\n",
    "        except: continue\n",
    "\n",
    "    if not clean:\n",
    "        print(\"未抓到论文！\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(clean).drop_duplicates(subset=[\"title\"])\n",
    "    df = df[df[\"year\"] >= MIN_YEAR]\n",
    "    df.sort_values(by=[\"citations\", \"year\"], ascending=[False, False], inplace=True)\n",
    "    df_top = df.head(TOP_N).copy()\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d\")\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"pointcloud_dl_top{TOP_N}_{ts}.csv\")\n",
    "    bib_path = os.path.join(OUTPUT_DIR, f\"pointcloud_dl_top{TOP_N}_{ts}.bib\")\n",
    "    df_top.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # BibTeX\n",
    "    db = BibDatabase()\n",
    "    for _, row in df_top.iterrows():\n",
    "        author_str = row[\"authors\"].replace(\", \", \" and \")\n",
    "        first = \"\".join(c for c in row[\"authors\"].split(\",\")[0].lower() if c.isalnum()) or \"unknown\"\n",
    "        entry = {\n",
    "            \"ENTRYTYPE\": \"misc\", \"ID\": f\"{first}{row['year']}\", \"title\": row[\"title\"],\n",
    "            \"author\": author_str, \"year\": str(row[\"year\"]), \"url\": row[\"pdf_link\"],\n",
    "            \"note\": f\"[{row['source']}] Citations: {row['citations']}\"\n",
    "        }\n",
    "        if \"arxiv\" in row[\"source\"].lower():\n",
    "            entry[\"howpublished\"] = f\"\\\\url{{{row['pdf_link']}}}\"\n",
    "        else:\n",
    "            entry[\"journal\"] = row[\"venue\"]\n",
    "        db.entries.append(entry)\n",
    "    with open(bib_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(BibTexWriter().write(db))\n",
    "\n",
    "    # 下载 PDF\n",
    "    print(f\"\\n开始下载 Top {TOP_N} 论文 PDF...\")\n",
    "    success = 0\n",
    "    for _, row in tqdm(df_top.iterrows(), total=len(df_top), desc=\"下载 PDF\"):\n",
    "        url = row[\"pdf_link\"]\n",
    "        if not url: continue\n",
    "        safe_title = safe_filename(row[\"title\"])\n",
    "        safe_auth = safe_filename(row[\"authors\"].split(\",\")[0])\n",
    "        filename = f\"{row['year']}_{safe_auth}_{safe_title}.pdf\"\n",
    "        filepath = os.path.join(PDF_DIR, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            success += 1\n",
    "            continue\n",
    "        if download_pdf(url, filepath):\n",
    "            success += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    # # 打包 PDF\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for file in os.listdir(PDF_DIR):\n",
    "            zf.write(os.path.join(PDF_DIR, file), file)\n",
    "\n",
    "    # print(f\"PDF 下载完成：{success}/{len(df_top)} 篇\")\n",
    "\n",
    "    # 发送邮件\n",
    "    print(f\"\\n正在发送邮件到 {EMAIL_RECEIVER}...\")\n",
    "    send_email_with_attachments(csv_path, bib_path, ZIP_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "496de600bfa26821",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始检索 Google Scholar...\n",
      "  查询: \"ICESat-2\" (\"point cloud\" OR \"photon cloud\" OR lidar OR \"laser altimetry\")\n",
      "  Scholar 第 1 页累计 10 篇\n",
      "  Scholar 第 2 页累计 19 篇\n",
      "  Scholar 第 3 页累计 26 篇\n",
      "  Scholar 第 4 页累计 33 篇\n",
      "  Scholar 第 5 页累计 40 篇\n",
      "  查询: \"ICESat-2\" \"deep learning\" (classification OR segmentation OR registration OR denoising OR bathymetry)\n",
      "  Scholar 第 1 页累计 7 篇\n",
      "  Scholar 第 2 页累计 12 篇\n",
      "  Scholar 第 3 页累计 17 篇\n",
      "  Scholar 第 4 页累计 21 篇\n",
      "  Scholar 第 5 页累计 25 篇\n",
      "  查询: \"photon point cloud\" \"deep learning\" (classification OR segmentation OR denoising OR bathymetry)\n",
      "  Scholar 第 1 页累计 1 篇\n",
      "  Scholar 第 2 页累计 1 篇\n",
      "  Scholar 第 3 页累计 4 篇\n",
      "  Scholar 第 4 页累计 8 篇\n",
      "  Scholar 第 5 页累计 8 篇\n",
      "  查询: \"satellite lidar\" OR \"spaceborne lidar\" (\"point cloud\" OR photon) \"deep learning\"\n",
      "  Scholar 第 1 页累计 7 篇\n",
      "  Scholar 第 2 页累计 11 篇\n",
      "  Scholar 第 3 页累计 18 篇\n",
      "  Scholar 第 4 页累计 19 篇\n",
      "  Scholar 第 5 页累计 19 篇\n",
      "  查询: \"ICESat-2\" (bathymetry OR \"water depth\" OR \"underwater topography\") \"deep learning\"\n",
      "  Scholar 第 1 页累计 7 篇\n",
      "  Scholar 第 2 页累计 12 篇\n",
      "  Scholar 第 3 页累计 17 篇\n",
      "  Scholar 第 4 页累计 21 篇\n",
      "  Scholar 第 5 页累计 22 篇\n",
      "  查询: \"ICESat-2\" (\"PointNet\" OR \"PointNet++\" OR \"RandLA-Net\" OR \"Point Transformer\" OR DGCNN OR KPConv)\n",
      "  Scholar 第 1 页累计 3 篇\n",
      "  Scholar 第 2 页累计 4 篇\n",
      "  Scholar 第 3 页累计 6 篇\n",
      "  Scholar 第 4 页累计 6 篇\n",
      "  Scholar 第 5 页累计 6 篇\n",
      "  查询: \"ICESat-2\" (denoising OR \"signal extraction\" OR \"photon classification\") \"deep learning\"\n",
      "  Scholar 第 1 页累计 3 篇\n",
      "  Scholar 第 2 页累计 6 篇\n",
      "  Scholar 第 3 页累计 10 篇\n",
      "  Scholar 第 4 页累计 14 篇\n",
      "  Scholar 第 5 页累计 17 篇\n",
      "\n",
      "开始检索 arXiv...\n",
      "  查询: ICESat-2 point cloud OR photon cloud OR lidar\n",
      "  arXiv 第 1 页累计 0 篇\n",
      "  arXiv 第 2 页累计 1 篇\n",
      "  arXiv 第 3 页累计 2 篇\n",
      "  arXiv 第 4 页累计 2 篇\n",
      "  查询: ICESat-2 deep learning classification OR segmentation OR bathymetry\n",
      "  arXiv 第 1 页累计 0 篇\n",
      "  arXiv 第 2 页累计 1 篇\n",
      "  arXiv 第 3 页累计 1 篇\n",
      "  arXiv 第 4 页累计 2 篇\n",
      "  arXiv 第 5 页累计 2 篇\n",
      "  查询: photon point cloud deep learning\n",
      "  arXiv 第 1 页累计 2 篇\n",
      "  arXiv 第 2 页累计 5 篇\n",
      "  arXiv 第 3 页累计 8 篇\n",
      "  arXiv 第 4 页累计 10 篇\n",
      "  arXiv 第 5 页累计 13 篇\n",
      "  查询: satellite lidar OR spaceborne lidar point cloud deep learning\n",
      "  arXiv 第 1 页累计 1 篇\n",
      "  arXiv 第 2 页累计 4 篇\n",
      "  arXiv 第 3 页累计 8 篇\n",
      "  arXiv 第 4 页累计 9 篇\n",
      "  arXiv 第 5 页累计 13 篇\n",
      "  查询: ICESat-2 bathymetry OR water depth deep learning\n",
      "  arXiv 第 1 页累计 0 篇\n",
      "  arXiv 第 2 页累计 1 篇\n",
      "  arXiv 第 3 页累计 2 篇\n",
      "  查询: ICESat-2 PointNet OR PointNet++ OR RandLA-Net OR Point Transformer\n",
      "  arXiv 第 1 页累计 0 篇\n",
      "  arXiv 第 2 页累计 1 篇\n",
      "  arXiv 第 3 页累计 1 篇\n",
      "  查询: ICESat-2 denoising OR photon classification deep learning\n",
      "  arXiv 第 1 页累计 1 篇\n",
      "  arXiv 第 2 页累计 1 篇\n",
      "\n",
      "开始下载 Top 100 论文 PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "下载 PDF: 100%|██████████| 100/100 [06:33<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在发送邮件到 zhouyusen@nuaa.edu.cn...\n",
      "正在使用 QQ 邮箱发送（第 1 次）...\n",
      "失败: (552, b'Message too large. http://service.mail.qq.com/cgi-bin/help?subtype=1&&id=20022&&no=1000729', '2117735297@qq.com')\n",
      "正在使用 QQ 邮箱发送（第 2 次）...\n",
      "失败: (552, b'Message too large. http://service.mail.qq.com/cgi-bin/help?subtype=1&&id=20022&&no=1000729', '2117735297@qq.com')\n",
      "正在使用 QQ 邮箱发送（第 3 次）...\n",
      "失败: (552, b'Message too large. http://service.mail.qq.com/cgi-bin/help?subtype=1&&id=20022&&no=1000729', '2117735297@qq.com')\n",
      "发送失败！请检查 QQ 邮箱授权码\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T05:40:55.297072Z",
     "start_time": "2025-10-26T05:40:53.771558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import zipfile\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "# ==================== 配置区 ====================\n",
    "SERPAPI_KEY = \"63eff930ec8b25bae49e14f4d4fd88bd19ab60d118cbf644e63082814710aede\"\n",
    "\n",
    "QUERIES = {\n",
    "    \"scholar\": ['\"point cloud\" \"deep learning\" (classification OR segmentation OR registration)'],\n",
    "    \"arxiv\": [\"point cloud deep learning\"]\n",
    "}\n",
    "\n",
    "MIN_YEAR = 2016\n",
    "MIN_CITATIONS_SCHOLAR = 50\n",
    "MAX_PAGES = {\"scholar\": 1, \"arxiv\": 1}  # 测试用\n",
    "TOP_N = 5  # 测试用\n",
    "\n",
    "# 新 QQ 邮箱（必须）\n",
    "EMAIL_SENDER = \"2117735297@qq.com\"          # 替换成你的新 QQ 邮箱\n",
    "EMAIL_PASSWORD = \"ebqipkzjqifbfcfi\"        # 替换成你的授权码\n",
    "EMAIL_RECEIVER = \"zhouyusen@nuaa.edu.cn\"\n",
    "\n",
    "OUTPUT_DIR = \"retrieved_papers\"\n",
    "PDF_DIR = os.path.join(OUTPUT_DIR, \"pdfs\")\n",
    "ZIP_PATH = os.path.join(OUTPUT_DIR, \"pdfs.zip\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------- 邮件发送 -------------------\n",
    "def send_email_with_attachments(csv_path, bib_path, zip_path):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_SENDER\n",
    "    msg['To'] = EMAIL_RECEIVER\n",
    "    msg['Subject'] = f\"【点云论文】Top {TOP_N} 高被引论文- {datetime.now().strftime('%Y%m%d')}\"\n",
    "\n",
    "    body = f\"附件为 Top {TOP_N} 篇高被引论文。\"\n",
    "    msg.attach(MIMEText(body, 'plain', 'utf-8'))\n",
    "\n",
    "    for file_path in [csv_path, bib_path, zip_path]:\n",
    "        if not os.path.exists(file_path): continue\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            part = MIMEBase('application', 'octet-stream')\n",
    "            part.set_payload(f.read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header('Content-Disposition', f'attachment; filename= {os.path.basename(file_path)}')\n",
    "            msg.attach(part)\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            print(f\"正在使用 QQ 邮箱发送（第 {attempt+1} 次）...\")\n",
    "            server = smtplib.SMTP(\"smtp.qq.com\", 587, timeout=20)\n",
    "            server.ehlo()\n",
    "            server.starttls()\n",
    "            server.ehlo()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.sendmail(EMAIL_SENDER, EMAIL_RECEIVER, msg.as_string())\n",
    "            server.quit()\n",
    "            print(f\"邮件发送成功！\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"失败: {e}\")\n",
    "            time.sleep(3)\n",
    "    print(\"发送失败！请检查 QQ 邮箱授权码\")\n",
    "\n",
    "# ------------------- 主函数 -------------------\n",
    "def main():\n",
    "    ts = datetime.now().strftime(\"%Y%m%d\")\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"test_{ts}.csv\")\n",
    "    bib_path = os.path.join(OUTPUT_DIR, f\"test_{ts}.bib\")\n",
    "    pd.DataFrame([{\"test\": 1}]).to_csv(csv_path, index=False)\n",
    "\n",
    "    db = BibDatabase()\n",
    "    db.entries = [{\"ENTRYTYPE\": \"misc\", \"ID\": \"test\", \"title\": \"test\"}]\n",
    "    with open(bib_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(BibTexWriter().write(db))\n",
    "\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'w') as zf:\n",
    "        zf.write(csv_path, os.path.basename(csv_path))\n",
    "\n",
    "    print(f\"\\n正在发送测试邮件到 {EMAIL_RECEIVER}...\")\n",
    "    send_email_with_attachments(csv_path, bib_path, ZIP_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "5444c9b182b4dca6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在发送测试邮件到 zhouyusen@nuaa.edu.cn...\n",
      "正在使用 QQ 邮箱发送（第 1 次）...\n",
      "邮件发送成功！\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ICESat-2 + 点云 + 深度学习 全平台检索器（终极版）\n",
    "支持：Google Scholar / arXiv / Web of Science / CNKI / Scopus\n",
    "输出：高亮 CSV + BibTeX + PDF + 邮件\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import bibtexparser\n",
    "from bibtexparser.bwriter import BibTexWriter\n",
    "from bibtexparser.bibdatabase import BibDatabase\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import zipfile\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "# ==================== 配置区 ====================\n",
    "SERPAPI_KEY = \"63eff930ec8b25bae49e14f4d4fd88bd19ab60d118cbf644e63082814710aede\"\n",
    "\n",
    "QUERIES = {\n",
    "    \"scholar\": [\n",
    "        '\"ICESat-2\" (\"point cloud\" OR \"photon cloud\" OR lidar OR \"laser altimetry\")',\n",
    "        '\"ICESat-2\" \"deep learning\" (classification OR segmentation OR denoising OR bathymetry)',\n",
    "        '\"photon point cloud\" \"deep learning\"',\n",
    "        '\"ICESat-2\" (bathymetry OR \"water depth\") \"deep learning\"',\n",
    "        '\"ICESat-2\" (\"PointNet\" OR \"PointNet++\" OR \"RandLA-Net\" OR \"Point Transformer\")'\n",
    "    ],\n",
    "    \"arxiv\": [\n",
    "        \"ICESat-2 point cloud OR photon cloud OR lidar\",\n",
    "        \"ICESat-2 deep learning classification OR segmentation OR bathymetry\",\n",
    "        \"photon point cloud deep learning\",\n",
    "        \"ICESat-2 PointNet OR PointNet++ OR RandLA-Net\"\n",
    "    ],\n",
    "    \"wos\": [\n",
    "        \"TS=(\\\"ICESat-2\\\" AND (\\\"point cloud\\\" OR \\\"photon cloud\\\" OR lidar))\",\n",
    "        \"TS=(\\\"ICESat-2\\\" AND \\\"deep learning\\\" AND (classification OR segmentation OR bathymetry))\",\n",
    "        \"TS=(\\\"photon point cloud\\\" AND \\\"deep learning\\\")\",\n",
    "        \"TS=(\\\"ICESat-2\\\" AND (bathymetry OR \\\"water depth\\\"))\"\n",
    "    ],\n",
    "    \"cnki\": [\n",
    "        \"SU='ICESat-2' AND (SU='点云' OR SU='光子点云' OR SU='激光雷达')\",\n",
    "        \"SU='ICESat-2' AND SU='深度学习' AND (SU='分类' OR SU='分割' OR SU='测深')\",\n",
    "        \"SU='光子点云' AND SU='深度学习'\",\n",
    "        \"SU='ICESat-2' AND (SU='测深' OR SU='水深')\"\n",
    "    ],\n",
    "    \"scopus\": [\n",
    "        \"TITLE-ABS-KEY ( \\\"ICESat-2\\\" AND ( \\\"point cloud\\\" OR \\\"photon cloud\\\" OR lidar ) )\",\n",
    "        \"TITLE-ABS-KEY ( \\\"ICESat-2\\\" AND \\\"deep learning\\\" AND ( classification OR segmentation OR bathymetry ) )\",\n",
    "        \"TITLE-ABS-KEY ( \\\"photon point cloud\\\" AND \\\"deep learning\\\" )\",\n",
    "        \"TITLE-ABS-KEY ( \\\"ICESat-2\\\" AND ( bathymetry OR \\\"water depth\\\" ) )\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "MIN_YEAR = 2018\n",
    "MIN_CITATIONS_SCHOLAR = 10\n",
    "MAX_PAGES = {\"scholar\": 3, \"arxiv\": 3, \"wos\": 2, \"cnki\": 2, \"scopus\": 2}\n",
    "TOP_N = 100\n",
    "\n",
    "# 关键词高亮（支持中英文）\n",
    "HIGHLIGHT_KEYWORDS = [\n",
    "    \"ICESat-2\", \"point cloud\", \"photon cloud\", \"lidar\", \"deep learning\",\n",
    "    \"classification\", \"segmentation\", \"bathymetry\", \"denoising\",\n",
    "    \"PointNet\", \"PointNet++\", \"RandLA-Net\", \"Point Transformer\",\n",
    "    \"点云\", \"光子点云\", \"激光雷达\", \"深度学习\", \"分类\", \"分割\", \"测深\"\n",
    "]\n",
    "\n",
    "EMAIL_SENDER = \"2117735297@qq.com\"\n",
    "EMAIL_PASSWORD = \"ebqipkzjqifbfcfi\"\n",
    "EMAIL_RECEIVER = \"zhouyusen@nuaa.edu.cn\"\n",
    "\n",
    "OUTPUT_DIR = \"retrieved_papers\"\n",
    "PDF_DIR = os.path.join(OUTPUT_DIR, \"pdfs\")\n",
    "ZIP_PATH = os.path.join(OUTPUT_DIR, \"pdfs.zip\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "\n",
    "# ===============================================\n",
    "\n",
    "def highlight_text(text, keywords):\n",
    "    if not text or pd.isna(text): return text\n",
    "    txt = str(text)\n",
    "    for kw in keywords:\n",
    "        txt = re.sub(f\"({re.escape(kw)})\", r\"<b>\\1</b>\", txt, flags=re.IGNORECASE)\n",
    "    return txt\n",
    "\n",
    "def random_sleep(min_sec=2, max_sec=5):\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "def safe_filename(name):\n",
    "    name = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', '_', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name[:100]\n",
    "\n",
    "def download_pdf(url, filepath, retries=3):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers, timeout=30, stream=True)\n",
    "            if resp.status_code == 200:\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in resp.iter_content(1024*1024):\n",
    "                        f.write(chunk)\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"  [重试 {attempt+1}] {e}\")\n",
    "            time.sleep(2)\n",
    "    return False\n",
    "\n",
    "# ------------------- Google Scholar -------------------\n",
    "def crawl_google_scholar_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"google_scholar\", \"q\": query, \"hl\": \"en\",\n",
    "            \"start\": page * 10, \"num\": 10, \"as_ylo\": MIN_YEAR, \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"organic_results\" not in data: break\n",
    "            for item in data[\"organic_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"snippet\", \"\")\n",
    "                    cited = 0\n",
    "                    if item.get(\"inline_links\", {}).get(\"cited_by\"):\n",
    "                        txt = item[\"inline_links\"][\"cited_by\"].get(\"total\", \"0\")\n",
    "                        cited = int(\"\".join(filter(str.isdigit, str(txt)))) if txt else 0\n",
    "                    year = None\n",
    "                    pub_info = item.get(\"publication_info\", {}).get(\"summary\", \"\")\n",
    "                    if isinstance(pub_info, (int, float)): pub_info = str(int(pub_info))\n",
    "                    elif not isinstance(pub_info, str): pub_info = str(pub_info)\n",
    "                    for token in pub_info.replace(\",\", \" \").split():\n",
    "                        if token.isdigit() and 1900 < int(token) < 2100:\n",
    "                            year = int(token); break\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        parts = pub_info.split(\" - \", 1)\n",
    "                        authors = parts[0].strip() if len(parts) > 0 else \"Unknown\"\n",
    "                        venue = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": venue,\n",
    "                            \"citations\": cited, \"abstract\": snippet[:500], \"pdf_link\": link,\n",
    "                            \"source\": \"Google Scholar\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  Scholar 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- arXiv -------------------\n",
    "def crawl_arxiv_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"google_scholar\", \"q\": f\"{query} source:arxiv\", \"hl\": \"en\",\n",
    "            \"start\": page * 10, \"num\": 10, \"as_ylo\": MIN_YEAR, \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"organic_results\" not in data: break\n",
    "            for item in data[\"organic_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"snippet\", \"\")\n",
    "                    cited = 0\n",
    "                    if item.get(\"inline_links\", {}).get(\"cited_by\"):\n",
    "                        txt = item[\"inline_links\"][\"cited_by\"].get(\"total\", \"0\")\n",
    "                        cited = int(\"\".join(filter(str.isdigit, str(txt)))) if txt else 0\n",
    "                    year = None\n",
    "                    pub_info = item.get(\"publication_info\", {}).get(\"summary\", \"\")\n",
    "                    if isinstance(pub_info, (int, float)): pub_info = str(int(pub_info))\n",
    "                    elif not isinstance(pub_info, str): pub_info = str(pub_info)\n",
    "                    for token in pub_info.replace(\",\", \" \").split():\n",
    "                        if token.isdigit() and 1900 < int(token) < 2100:\n",
    "                            year = int(token); break\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        authors = \"Unknown\"\n",
    "                        if pub_info:\n",
    "                            parts = pub_info.split(\" - \", 1)\n",
    "                            authors = parts[0].strip() if len(parts) > 0 else \"Unknown\"\n",
    "                        pdf_link = \"\"\n",
    "                        if \"arxiv.org/abs/\" in link:\n",
    "                            arxiv_id = link.split(\"/abs/\")[-1].split(\"?\")[0].split(\"#\")[0]\n",
    "                            pdf_link = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "                        elif \"arxiv.org/pdf/\" in link:\n",
    "                            pdf_link = link\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": \"arXiv\",\n",
    "                            \"citations\": cited, \"abstract\": snippet[:500], \"pdf_link\": pdf_link or link,\n",
    "                            \"source\": \"arXiv\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  arXiv 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- Web of Science -------------------\n",
    "def crawl_wos_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"web_of_science\",\n",
    "            \"q\": query,\n",
    "            \"page\": page + 1,\n",
    "            \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"search_results\" not in data: break\n",
    "            for item in data[\"search_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"abstract\", \"\")[:500]\n",
    "                    cited = int(item.get(\"cited_by\", 0)) if item.get(\"cited_by\") else 0\n",
    "                    year = int(item.get(\"year\", 0)) if item.get(\"year\") else None\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        authors = item.get(\"authors\", \"Unknown\")\n",
    "                        venue = item.get(\"source\", \"WoS\")\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": venue,\n",
    "                            \"citations\": cited, \"abstract\": snippet, \"pdf_link\": link,\n",
    "                            \"source\": \"Web of Science\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  WoS 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- CNKI -------------------\n",
    "def crawl_cnki_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"cnki\",\n",
    "            \"q\": query,\n",
    "            \"page\": page + 1,\n",
    "            \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"results\" not in data: break\n",
    "            for item in data[\"results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"abstract\", \"\")[:500]\n",
    "                    cited = int(item.get(\"cited_count\", 0)) if item.get(\"cited_count\") else 0\n",
    "                    year_str = item.get(\"year\", \"\")\n",
    "                    year = int(year_str) if year_str.isdigit() else None\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        authors = item.get(\"authors\", \"Unknown\")\n",
    "                        venue = item.get(\"journal\", \"CNKI\")\n",
    "                        pdf_link = link\n",
    "                        if \"kns\" in link and \"pdf\" not in link:\n",
    "                            pdf_link = link.replace(\"detail\", \"pdf\")\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": venue,\n",
    "                            \"citations\": cited, \"abstract\": snippet, \"pdf_link\": pdf_link,\n",
    "                            \"source\": \"CNKI\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  CNKI 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- Scopus -------------------\n",
    "def crawl_scopus_api(query, max_pages=3, api_key=None):\n",
    "    if not api_key: return []\n",
    "    papers = []\n",
    "    base_url = \"https://serpapi.com/search\"\n",
    "    for page in range(max_pages):\n",
    "        params = {\n",
    "            \"engine\": \"scopus\",\n",
    "            \"q\": query,\n",
    "            \"start\": page * 25,\n",
    "            \"count\": 25,\n",
    "            \"api_key\": api_key\n",
    "        }\n",
    "        try:\n",
    "            data = requests.get(base_url, params=params, timeout=30).json()\n",
    "            if \"search_results\" not in data: break\n",
    "            for item in data[\"search_results\"]:\n",
    "                try:\n",
    "                    title = item.get(\"title\", \"Unknown\")\n",
    "                    link = item.get(\"link\", \"\")\n",
    "                    snippet = item.get(\"abstract\", \"\")[:500]\n",
    "                    cited = int(item.get(\"citedby_count\", 0)) if item.get(\"citedby_count\") else 0\n",
    "                    year = int(item.get(\"cover_date\", \"\")[:4]) if item.get(\"cover_date\") else None\n",
    "                    if year and year >= MIN_YEAR and cited >= MIN_CITATIONS_SCHOLAR:\n",
    "                        authors = item.get(\"creator\", \"Unknown\")\n",
    "                        venue = item.get(\"publication_name\", \"Scopus\")\n",
    "                        papers.append({\n",
    "                            \"title\": title, \"authors\": authors, \"year\": year, \"venue\": venue,\n",
    "                            \"citations\": cited, \"abstract\": snippet, \"pdf_link\": link,\n",
    "                            \"source\": \"Scopus\"\n",
    "                        })\n",
    "                except: continue\n",
    "            print(f\"  Scopus 第 {page+1} 页累计 {len(papers)} 篇\")\n",
    "            time.sleep(2)\n",
    "        except: break\n",
    "    return papers\n",
    "\n",
    "# ------------------- 邮件发送 -------------------\n",
    "def send_email_with_attachments(csv_path, bib_path, zip_path):\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = EMAIL_SENDER\n",
    "    msg['To'] = EMAIL_RECEIVER\n",
    "    msg['Subject'] = f\"【ICESat-2论文】Top {TOP_N} 高被引 - {datetime.now().strftime('%Y%m%d')}\"\n",
    "\n",
    "    body = f\"附件为 5 大平台检索的 Top {TOP_N} 篇高被引论文（含关键词高亮）。\"\n",
    "    msg.attach(MIMEText(body, 'plain', 'utf-8'))\n",
    "\n",
    "    for file_path in [csv_path, bib_path, zip_path]:\n",
    "        if not os.path.exists(file_path): continue\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            part = MIMEBase('application', 'octet-stream')\n",
    "            part.set_payload(f.read())\n",
    "            encoders.encode_base64(part)\n",
    "            part.add_header('Content-Disposition', f'attachment; filename= {os.path.basename(file_path)}')\n",
    "            msg.attach(part)\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            print(f\"正在发送邮件（第 {attempt+1} 次）...\")\n",
    "            server = smtplib.SMTP(\"smtp.qq.com\", 587, timeout=20)\n",
    "            server.ehlo()\n",
    "            server.starttls()\n",
    "            server.ehlo()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.sendmail(EMAIL_SENDER, EMAIL_RECEIVER, msg.as_string())\n",
    "            server.quit()\n",
    "            print(f\"邮件发送成功！\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"失败: {e}\")\n",
    "            time.sleep(3)\n",
    "    print(\"邮件发送失败！\")\n",
    "\n",
    "# ------------------- 主函数 -------------------\n",
    "def main():\n",
    "    all_papers = []\n",
    "\n",
    "    print(\"开始检索 Google Scholar...\")\n",
    "    for q in QUERIES[\"scholar\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_google_scholar_api(q, MAX_PAGES[\"scholar\"], SERPAPI_KEY))\n",
    "\n",
    "    print(\"\\n开始检索 arXiv...\")\n",
    "    for q in QUERIES[\"arxiv\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_arxiv_api(q, MAX_PAGES[\"arxiv\"], SERPAPI_KEY))\n",
    "\n",
    "    print(\"\\n开始检索 Web of Science...\")\n",
    "    for q in QUERIES[\"wos\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_wos_api(q, MAX_PAGES.get(\"wos\", 2), SERPAPI_KEY))\n",
    "\n",
    "    print(\"\\n开始检索 中国知网...\")\n",
    "    for q in QUERIES[\"cnki\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_cnki_api(q, MAX_PAGES.get(\"cnki\", 2), SERPAPI_KEY))\n",
    "\n",
    "    print(\"\\n开始检索 Scopus...\")\n",
    "    for q in QUERIES[\"scopus\"]:\n",
    "        print(f\"  查询: {q}\")\n",
    "        all_papers.extend(crawl_scopus_api(q, MAX_PAGES.get(\"scopus\", 2), SERPAPI_KEY))\n",
    "\n",
    "    # 清洗\n",
    "    clean = []\n",
    "    for p in all_papers:\n",
    "        try:\n",
    "            clean.append({\n",
    "                \"title\": str(p.get(\"title\", \"Unknown\")),\n",
    "                \"authors\": str(p.get(\"authors\", \"Unknown\")),\n",
    "                \"year\": int(p.get(\"year\") or 0),\n",
    "                \"venue\": str(p.get(\"venue\", \"Unknown\")),\n",
    "                \"citations\": int(p.get(\"citations\") or 0),\n",
    "                \"abstract\": str(p.get(\"abstract\", \"\"))[:500],\n",
    "                \"pdf_link\": str(p.get(\"pdf_link\", \"\")),\n",
    "                \"source\": str(p.get(\"source\", \"Unknown\"))\n",
    "            })\n",
    "        except: continue\n",
    "\n",
    "    if not clean:\n",
    "        print(\"未抓到论文！\")\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(clean).drop_duplicates(subset=[\"title\"])\n",
    "    df = df[df[\"year\"] >= MIN_YEAR]\n",
    "    df.sort_values(by=[\"citations\", \"year\"], ascending=[False, False], inplace=True)\n",
    "    df_top = df.head(TOP_N).copy()\n",
    "\n",
    "    # 关键词高亮\n",
    "    df_top[\"title_hl\"] = df_top[\"title\"].apply(lambda x: highlight_text(x, HIGHLIGHT_KEYWORDS))\n",
    "    df_top[\"abstract_hl\"] = df_top[\"abstract\"].apply(lambda x: highlight_text(x, HIGHLIGHT_KEYWORDS))\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d\")\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"icesat2_highlight_top{TOP_N}_{ts}.csv\")\n",
    "    bib_path = os.path.join(OUTPUT_DIR, f\"icesat2_top{TOP_N}_{ts}.bib\")\n",
    "\n",
    "    # 保存高亮 CSV（可用 Excel 打开看到 <b> 加粗）\n",
    "    df_top[[\"title_hl\", \"authors\", \"year\", \"venue\", \"citations\", \"source\", \"abstract_hl\", \"pdf_link\"]].to_csv(\n",
    "        csv_path, index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    # BibTeX\n",
    "    db = BibDatabase()\n",
    "    for _, row in df_top.iterrows():\n",
    "        author_str = row[\"authors\"].replace(\", \", \" and \")\n",
    "        first = \"\".join(c for c in row[\"authors\"].split(\",\")[0].lower() if c.isalnum()) or \"unknown\"\n",
    "        entry = {\n",
    "            \"ENTRYTYPE\": \"misc\", \"ID\": f\"{first}{row['year']}\", \"title\": row[\"title\"],\n",
    "            \"author\": author_str, \"year\": str(row[\"year\"]), \"url\": row[\"pdf_link\"],\n",
    "            \"note\": f\"[{row['source']}] Citations: {row['citations']}\"\n",
    "        }\n",
    "        if \"arxiv\" in row[\"source\"].lower():\n",
    "            entry[\"howpublished\"] = f\"\\\\url{{{row['pdf_link']}}}\"\n",
    "        else:\n",
    "            entry[\"journal\"] = row[\"venue\"]\n",
    "        db.entries.append(entry)\n",
    "    with open(bib_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(BibTexWriter().write(db))\n",
    "\n",
    "    # 下载 PDF\n",
    "    print(f\"\\n开始下载 Top {TOP_N} 论文 PDF...\")\n",
    "    success = 0\n",
    "    for _, row in tqdm(df_top.iterrows(), total=len(df_top), desc=\"下载 PDF\"):\n",
    "        url = row[\"pdf_link\"]\n",
    "        if not url: continue\n",
    "        safe_title = safe_filename(row[\"title\"])\n",
    "        safe_auth = safe_filename(row[\"authors\"].split(\",\")[0])\n",
    "        filename = f\"{row['year']}_{safe_auth}_{safe_title}.pdf\"\n",
    "        filepath = os.path.join(PDF_DIR, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            success += 1\n",
    "            continue\n",
    "        if download_pdf(url, filepath):\n",
    "            success += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    # 打包\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for file in os.listdir(PDF_DIR):\n",
    "            zf.write(os.path.join(PDF_DIR, file), file)\n",
    "\n",
    "    # 发送邮件\n",
    "    print(f\"\\n正在发送邮件到 {EMAIL_RECEIVER}...\")\n",
    "    send_email_with_attachments(csv_path, bib_path, ZIP_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "78d9502193b07a10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
